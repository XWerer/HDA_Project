{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import SpeechGenerator\n",
    "import librosa\n",
    "#from keras import losses\n",
    "\n",
    "#from extractMFCC import computeFeatures, computeFeatures1\n",
    "#from addNoise import addNoise\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a problem with the maximum lenght of a string then We save the Dataset root folder as a variable\n",
    "dataset_dir = \"Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor idx, item in enumerate(testWAVs):\\n    testWAVs[idx] = \"Dataset/\" + testWAVs[idx]\\n\\nfor idx, item in enumerate(valWAVs):\\n    valWAVs[idx] = \"Dataset/\" + valWAVs[idx]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWAVs = pd.read_csv(dataset_dir + 'testing_list.txt', sep=\" \", header=None)[0].tolist()\n",
    "valWAVs  = pd.read_csv(dataset_dir + 'validation_list.txt', sep=\" \", header=None)[0].tolist()\n",
    "\"\"\"\n",
    "for idx, item in enumerate(testWAVs):\n",
    "    testWAVs[idx] = \"Dataset/\" + testWAVs[idx]\n",
    "\n",
    "for idx, item in enumerate(valWAVs):\n",
    "    valWAVs[idx] = \"Dataset/\" + valWAVs[idx]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DictCategs = {'nine' : 1, 'yes' : 2, 'no' : 3, 'up' : 4, 'down' : 5, 'left' : 6, 'right' : 7, 'on' : 8, 'off' : 9, \n",
    "              'stop' : 10, 'go' : 11, 'zero' : 12, 'one' : 13, 'two' : 14, 'three' : 15, 'four' : 16, 'five' : 17, \n",
    "              'six' : 18, 'seven' : 19, 'eight' : 20, 'backward':0, 'bed':0, 'bird':0, 'cat':0, 'dog':0, 'follow':0, \n",
    "              'forward':0, 'happy':0, 'house':0, 'learn':0, 'marvin':0, 'sheila':0, 'tree':0, 'visual':0, 'wow':0 }\n",
    "nCategs = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWAVs  = []\n",
    "for root, dirs, files in os.walk('Dataset/'):\n",
    "    for f in files:\n",
    "        if (root != dataset_dir + \"_background_noise_\") and (f.endswith('.wav')):\n",
    "            path = root + \"/\" + f\n",
    "            #print(path)\n",
    "            path = path[len(dataset_dir):]\n",
    "            #print(path)\n",
    "            allWAVs.append(path)\n",
    "    \"\"\"\n",
    "    print(f for f in files if f.endswith('.wav'))\n",
    "    if root != dataset_dir + \"_background_noise_\":\n",
    "        allWAVs += [root+'/'+ f for f in files if f.endswith('.wav')]\n",
    "    \"\"\"\n",
    "trainWAVs = list(set(allWAVs) - set(valWAVs) - set(testWAVs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84843\n",
      "9981\n",
      "11005\n"
     ]
    }
   ],
   "source": [
    "print(len(trainWAVs))\n",
    "print(len(valWAVs))\n",
    "print(len(testWAVs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getFileCategory(file, catDict):\n",
    "    # Receives a file with name <cat>/<filename> and returns an integer that is catDict[cat]\n",
    "    categ = os.path.basename(os.path.dirname(file))\n",
    "    return catDict.get(categ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get categories\n",
    "testWAVlabels = [_getFileCategory(f, DictCategs) for f in testWAVs]\n",
    "valWAVlabels = [_getFileCategory(f, DictCategs) for f in valWAVs]\n",
    "trainWAVlabels = [_getFileCategory(f, DictCategs) for f in trainWAVs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84843\n",
      "9981\n",
      "11005\n"
     ]
    }
   ],
   "source": [
    "print(len(trainWAVlabels))\n",
    "print(len(valWAVlabels))\n",
    "print(len(testWAVlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def load_and_preprocess_data(file_name):\\n    # Required by tensorflow (strings are passed as bytes)\\n    # tf.strings.as_string(...)\\n    file_name = file_name.numpy()\\n    file_name = file_name.decode(\"utf-8\") \\n    #print(file_name)\\n    #print(type(file_name))\\n\\n    # Load data\\n    feats = computeFeatures(file_name)\\n    # Normalize\\n    mean = np.mean(feats, axis = 0)\\n    stv = np.std(feats, axis = 0)\\n    diff = np.subtract(feats, mean)\\n    feats = np.divide(diff, stv)\\n\\n    return feats.astype(np.float32)\\n\\n# example:\\n\\nfeats = load_and_preprocess_data(trainWAVs[25])\\nfeats = np.transpose(feats)\\n\\nplt.figure(figsize=(17,6))\\nplt.pcolormesh(feats)\\n\\nplt.title(\\'Spectrogram visualization\\')\\nplt.ylabel(\\'Frequency\\')\\nplt.xlabel(\\'Time\\')\\n\\nplt.show()\\nprint(trainWAVlabels[25])\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def load_and_preprocess_data(file_name):\n",
    "    # Required by tensorflow (strings are passed as bytes)\n",
    "    # tf.strings.as_string(...)\n",
    "    file_name = file_name.numpy()\n",
    "    file_name = file_name.decode(\"utf-8\") \n",
    "    #print(file_name)\n",
    "    #print(type(file_name))\n",
    "\n",
    "    # Load data\n",
    "    feats = computeFeatures(file_name)\n",
    "    # Normalize\n",
    "    mean = np.mean(feats, axis = 0)\n",
    "    stv = np.std(feats, axis = 0)\n",
    "    diff = np.subtract(feats, mean)\n",
    "    feats = np.divide(diff, stv)\n",
    "\n",
    "    return feats.astype(np.float32)\n",
    "\n",
    "# example:\n",
    "\n",
    "feats = load_and_preprocess_data(trainWAVs[25])\n",
    "feats = np.transpose(feats)\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.pcolormesh(feats)\n",
    "\n",
    "plt.title('Spectrogram visualization')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.show()\n",
    "print(trainWAVlabels[25])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def create_dataset(file_names, labels, batch_size, shuffle, cache_file=None):\\n    \\n    # Create a Dataset object\\n    dataset = tf.data.Dataset.from_tensor_slices((file_names, labels))\\n    \\n    # Map the load_and_preprocess_data function\\n    py_func = lambda file_name, label: (tf.py_function(load_and_preprocess_data, [file_name], Tout=tf.float32), label)\\n    dataset = dataset.map(py_func, num_parallel_calls=os.cpu_count())\\n    \\n    # Cache dataset\\n    if cache_file:\\n        dataset = dataset.cache(cache_file)\\n    \\n    # Shuffle    \\n    if shuffle:\\n        dataset = dataset.shuffle(len(file_names))\\n        \\n    # Repeat the dataset indefinitely\\n    dataset = dataset.repeat()\\n    \\n    # Correct input shape for the network\\n    dataset = dataset.map(lambda data, label: (tf.expand_dims(data, 1), label))\\n    \\n    # Batch\\n    dataset = dataset.batch(batch_size=batch_size)\\n    \\n    # Prefetch\\n    dataset = dataset.prefetch(buffer_size=1)\\n    \\n    return dataset\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def create_dataset(file_names, labels, batch_size, shuffle, cache_file=None):\n",
    "    \n",
    "    # Create a Dataset object\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_names, labels))\n",
    "    \n",
    "    # Map the load_and_preprocess_data function\n",
    "    py_func = lambda file_name, label: (tf.py_function(load_and_preprocess_data, [file_name], Tout=tf.float32), label)\n",
    "    dataset = dataset.map(py_func, num_parallel_calls=os.cpu_count())\n",
    "    \n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "    \n",
    "    # Shuffle    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(file_names))\n",
    "        \n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Correct input shape for the network\n",
    "    dataset = dataset.map(lambda data, label: (tf.expand_dims(data, 1), label))\n",
    "    \n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    \n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "    \n",
    "    return dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_size = 32\\ntrain_dataset = create_dataset(trainWAVs, \\n                               trainWAVlabels, \\n                               batch_size=batch_size, \\n                               shuffle=True,\\n                               cache_file='train_cache')\\n\\nval_dataset = create_dataset(valWAVs, \\n                             valWAVlabels,\\n                             batch_size=batch_size, \\n                             shuffle=False,\\n                             cache_file='val_cache')\\n\\ntrain_steps = int(np.ceil(len(trainWAVs)/batch_size))\\nval_steps = int(np.ceil(len(valWAVs)/batch_size))\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"batch_size = 32\n",
    "train_dataset = create_dataset(trainWAVs, \n",
    "                               trainWAVlabels, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True,\n",
    "                               cache_file='train_cache')\n",
    "\n",
    "val_dataset = create_dataset(valWAVs, \n",
    "                             valWAVlabels,\n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False,\n",
    "                             cache_file='val_cache')\n",
    "\n",
    "train_steps = int(np.ceil(len(trainWAVs)/batch_size))\n",
    "val_steps = int(np.ceil(len(valWAVs)/batch_size))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of the train set:\n",
      "0/84843\n",
      "5000/84843\n",
      "10000/84843\n",
      "15000/84843\n",
      "20000/84843\n",
      "25000/84843\n",
      "30000/84843\n",
      "35000/84843\n",
      "40000/84843\n",
      "45000/84843\n",
      "50000/84843\n",
      "55000/84843\n",
      "60000/84843\n",
      "65000/84843\n",
      "70000/84843\n",
      "75000/84843\n",
      "80000/84843\n",
      "84842/84843\n"
     ]
    }
   ],
   "source": [
    "# Transoform the dataset in numpy array and load \n",
    "train = np.array(trainWAVs, dtype = object)\n",
    "\n",
    "print(\"Loading of the train set:\")\n",
    "for i in range(len(trainWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(trainWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + trainWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + trainWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + trainWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    train[i] = trainWAVs[i] + '.npy'\n",
    "    \n",
    "print(str(i) + '/' + str(len(trainWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of the validation set:\n",
      "0/9981\n",
      "5000/9981\n",
      "9980/9981\n",
      "Loading of the test set:\n",
      "0/11005\n",
      "5000/11005\n",
      "10000/11005\n",
      "11004/11005\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for the validation and the test set\n",
    "val = np.array(valWAVs, dtype = object)\n",
    "\n",
    "print(\"Loading of the validation set:\")\n",
    "for i in range(len(valWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(valWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + valWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + valWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + valWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    val[i] = valWAVs[i] + '.npy'\n",
    "    \n",
    "print(str(i) + '/' + str(len(valWAVs)))\n",
    "\n",
    "test = np.array(testWAVs, dtype = object)\n",
    "\n",
    "print(\"Loading of the test set:\")\n",
    "for i in range(len(testWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(testWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + testWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + testWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + testWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    test[i] = testWAVs[i] + '.npy' \n",
    "    \n",
    "print(str(i) + '/' + str(len(testWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84843 = 84843\n",
      "9981 = 9981\n",
      "11005 = 11005\n",
      "file: eight/c86d4fd4_nohash_0.wav.npy - label: 20\n",
      "file: eight/c86d4fd4_nohash_0.wav - label: 20\n",
      "file: one/d926bb17_nohash_0.wav.npy - label: 13\n",
      "file: one/d926bb17_nohash_0.wav - label: 13\n",
      "file: nine/f575faf3_nohash_1.wav.npy - label: 1\n",
      "file: nine/f575faf3_nohash_1.wav - label: 1\n",
      "file: three/fbb2f6cc_nohash_1.wav.npy - label: 15\n",
      "file: three/fbb2f6cc_nohash_1.wav - label: 15\n",
      "file: tree/cdee383b_nohash_0.wav.npy - label: 0\n",
      "file: tree/cdee383b_nohash_0.wav - label: 0\n",
      "file: marvin/2f813234_nohash_0.wav.npy - label: 0\n",
      "file: marvin/2f813234_nohash_0.wav - label: 0\n",
      "file: two/e9b61425_nohash_0.wav.npy - label: 14\n",
      "file: two/e9b61425_nohash_0.wav - label: 14\n",
      "file: eight/61e2f74f_nohash_1.wav.npy - label: 20\n",
      "file: eight/61e2f74f_nohash_1.wav - label: 20\n",
      "file: house/3d6bee47_nohash_1.wav.npy - label: 0\n",
      "file: house/3d6bee47_nohash_1.wav - label: 0\n",
      "file: backward/a7acbbeb_nohash_0.wav.npy - label: 0\n",
      "file: backward/a7acbbeb_nohash_0.wav - label: 0\n",
      "file: no/7cb5c0b7_nohash_1.wav.npy - label: 3\n",
      "file: no/7cb5c0b7_nohash_1.wav - label: 3\n",
      "file: tree/ee07dcb9_nohash_0.wav.npy - label: 0\n",
      "file: tree/ee07dcb9_nohash_0.wav - label: 0\n",
      "file: nine/3bc21161_nohash_3.wav.npy - label: 1\n",
      "file: nine/3bc21161_nohash_3.wav - label: 1\n",
      "file: off/2b5e346d_nohash_0.wav.npy - label: 9\n",
      "file: off/2b5e346d_nohash_0.wav - label: 9\n",
      "file: five/682e1687_nohash_3.wav.npy - label: 17\n",
      "file: five/682e1687_nohash_3.wav - label: 17\n",
      "file: on/02e85b60_nohash_0.wav.npy - label: 8\n",
      "file: on/02e85b60_nohash_0.wav - label: 8\n",
      "file: one/33246bc2_nohash_0.wav.npy - label: 13\n",
      "file: one/33246bc2_nohash_0.wav - label: 13\n",
      "file: no/f68160c0_nohash_0.wav.npy - label: 3\n",
      "file: no/f68160c0_nohash_0.wav - label: 3\n",
      "file: dog/57376a4c_nohash_2.wav.npy - label: 0\n",
      "file: dog/57376a4c_nohash_2.wav - label: 0\n",
      "file: stop/773e26f7_nohash_4.wav.npy - label: 10\n",
      "file: stop/773e26f7_nohash_4.wav - label: 10\n",
      "file: yes/e4be0cf6_nohash_2.wav.npy - label: 2\n",
      "file: yes/e4be0cf6_nohash_2.wav - label: 2\n",
      "file: go/57152045_nohash_0.wav.npy - label: 11\n",
      "file: go/57152045_nohash_0.wav - label: 11\n",
      "file: down/eb0676ec_nohash_3.wav.npy - label: 5\n",
      "file: down/eb0676ec_nohash_3.wav - label: 5\n",
      "file: seven/f8ad3941_nohash_0.wav.npy - label: 19\n",
      "file: seven/f8ad3941_nohash_0.wav - label: 19\n",
      "file: tree/6846af18_nohash_0.wav.npy - label: 0\n",
      "file: tree/6846af18_nohash_0.wav - label: 0\n"
     ]
    }
   ],
   "source": [
    "#test on the length\n",
    "print(str(len(trainWAVs)) + \" = \" + str(len(train)))\n",
    "print(str(len(valWAVs)) + \" = \" + str(len(val)))\n",
    "print(str(len(testWAVs)) + \" = \" + str(len(test)))\n",
    "\n",
    "#test on labels \n",
    "for i in range(0, 25):\n",
    "    print(\"file: \" + train[i] + \" - label: \" + str(trainWAVlabels[i]))\n",
    "    print(\"file: \" + trainWAVs[i] + \" - label: \" + str(trainWAVlabels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = SpeechGenerator.SpeechGen(train, trainWAVlabels, dataset_dir, batch_size = 32, shuffle = True)\n",
    "valGen = SpeechGenerator.SpeechGen(val, valWAVlabels, dataset_dir, shuffle = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0625 17:37:03.459053 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0625 17:37:03.475083 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0625 17:37:03.618856 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0625 17:37:03.645856 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0625 17:37:03.672529 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0625 17:37:03.740078 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0625 17:37:04.028037 139676877416192 deprecation.py:506] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0625 17:37:05.125496 139676877416192 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 16000)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mel_stft (Melspectrogram)       (None, 80, 125, 1)   1091664     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "normalization2d_1 (Normalizatio (None, 80, 125, 1)   0           mel_stft[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 125, 80, 1)   0           normalization2d_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 80, 10)  60          permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 80, 10)  40          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 80, 1)   51          batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 80, 1)   4           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "squeeze_last_dim (Lambda)       (None, 125, 80)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 125, 120)     50760       squeeze_last_dim[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 125, 120)     65160       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 120)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 120)          14520       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 125)          0           dense_1[0][0]                    \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attSoftmax (Softmax)            (None, 125)          0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 120)          0           attSoftmax[0][0]                 \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           7744        dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 21)           693         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,232,776\n",
      "Trainable params: 141,090\n",
      "Non-trainable params: 1,091,686\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model.AttentionModel(21, 99, 60, use_GRU = True, dropout = 0.0, activation = 'relu')\n",
    "\"\"\"\n",
    "decayed_lr = tf.train.exponential_decay(0.0000001,\n",
    "                                        0.8, 50,\n",
    "                                        0.95, staircase=True)\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate=decayed_lr,  \n",
    "                                beta1=0.9,\n",
    "                                beta2=0.999,\n",
    "                                epsilon=1e-07,\n",
    "                                use_locking=False,\n",
    "                                name='Adam')\n",
    "\"\"\"\n",
    "#adam = tf.train.AdamOptimizer(learning_rate=0.00000001,  \n",
    "#                                beta1=0.9,\n",
    "#                                beta2=0.999,\n",
    "#                                epsilon=1e-07,\n",
    "#                                use_locking=False,\n",
    "#                                name='Adam')\n",
    "\n",
    "#model.compile(optimizer=adam,\n",
    "#              loss= tf.losses.softmax_cross_entropy,\n",
    "#              metrics=[keras.metrics.sparse_categorical_accuracy, ])\n",
    "\n",
    "model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['sparse_categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.4\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "            math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    if (lrate < 4e-5):\n",
    "        lrate = 4e-5\n",
    "      \n",
    "    print('Changing learning rate to {}'.format(lrate))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=10, verbose=1)\n",
    "checkpointer = ModelCheckpoint('model-attRNN.h5', monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0625 17:37:19.556581 139676877416192 deprecation.py:323] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Changing learning rate to 0.001\n",
      "2651/2651 [==============================] - 1065s 402ms/step - loss: 0.9805 - sparse_categorical_accuracy: 0.7101 - val_loss: nan - val_sparse_categorical_accuracy: 0.8740\n",
      "\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.87399, saving model to model-attRNN.h5\n",
      "Epoch 2/3\n",
      "Changing learning rate to 0.001\n",
      "2651/2651 [==============================] - 1076s 406ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8851 - val_loss: nan - val_sparse_categorical_accuracy: 0.8955\n",
      "\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.87399 to 0.89546, saving model to model-attRNN.h5\n",
      "Epoch 3/3\n",
      "Changing learning rate to 0.001\n",
      " 365/2651 [===>..........................] - ETA: 14:44 - loss: nan - sparse_categorical_accuracy: 0.2673"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0d978a68a30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainGen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalGen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystopper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model/my_model_sparse-loss_dropout-0_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "history = model.fit_generator(trainGen, validation_data = valGen, epochs = num_epochs, callbacks = [earlystopper, checkpointer, lrate])\n",
    "\n",
    "# Save the model\n",
    "model.save('Model/my_model_sparse-loss_dropout-0_1.h5')\n",
    "print(history.history.keys())\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='Train acc')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label='Val acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 47s 277ms/step\n",
      "loss: nan\n",
      "acc: 0.2580409356725146\n"
     ]
    }
   ],
   "source": [
    "testGen = SpeechGenerator.SpeechGen(test, testWAVlabels, dataset_dir, shuffle=False)\n",
    "\n",
    "testEval = model.evaluate_generator(testGen, verbose = 1)\n",
    "\n",
    "print(\"loss: \" + str(testEval[0]))\n",
    "print(\"acc: \" + str(testEval[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
