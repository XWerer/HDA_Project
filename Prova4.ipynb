{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/home/jupyter/.local/lib/python3.5/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d358f7fd3563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from tensorflow.keras.models import Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HDA_Project/Model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TensorFlow and tf.keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#from tensorflow import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;34m\"numpy in {}. One method of fixing this is to repeatedly uninstall \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"numpy until none is found, then reinstall this version.\")\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerictypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/home/jupyter/.local/lib/python3.5/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version."
     ]
    }
   ],
   "source": [
    "import Model\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "#from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import SpeechGenerator\n",
    "import librosa\n",
    "#from keras import losses\n",
    "\n",
    "from extractMFCC import computeFeatures, computeFeatures1\n",
    "from addNoise import addNoise\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "#print(tf.executing_eagerly())\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder of the dataset\n",
    "dataset_dir = \"Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing the path that identify the test and validation set\n",
    "testWAVs = pd.read_csv(dataset_dir + 'testing_list.txt', sep=\" \", header = None)[0].tolist()\n",
    "valWAVs  = pd.read_csv(dataset_dir + 'validation_list.txt', sep=\" \", header = None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing the mapping between category name and label\n",
    "DictCategs = {'nine' : 1, 'yes' : 2, 'no' : 3, 'up' : 4, 'down' : 5, 'left' : 6, 'right' : 7, 'on' : 8, 'off' : 9, \n",
    "              'stop' : 10, 'go' : 11, 'zero' : 12, 'one' : 13, 'two' : 14, 'three' : 15, 'four' : 16, 'five' : 17, \n",
    "              'six' : 18, 'seven' : 19, 'eight' : 20, 'backward':0, 'bed':0, 'bird':0, 'cat':0, 'dog':0, 'follow':0, \n",
    "              'forward':0, 'happy':0, 'house':0, 'learn':0, 'marvin':0, 'sheila':0, 'tree':0, 'visual':0, 'wow':0 }\n",
    "nCategs = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the file in dataset\n",
    "allWAVs  = []\n",
    "for root, dirs, files in os.walk('Dataset/'):\n",
    "    for f in files:\n",
    "        if (root != dataset_dir + \"_background_noise_\") and (f.endswith('.wav')):\n",
    "            path = root + \"/\" + f\n",
    "            #print(path)\n",
    "            path = path[len(dataset_dir):]\n",
    "            #print(path)\n",
    "            allWAVs.append(path)\n",
    "\n",
    "# Remove from the training set the elements present in test and validation\n",
    "trainWAVs = list(set(allWAVs) - set(valWAVs) - set(testWAVs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of sets\n",
    "print(\"Train set length: \" + str(len(trainWAVs)))\n",
    "print(\"Validation set length: \" + str(len(valWAVs)))\n",
    "print(\"Test set length: \" + str(len(testWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the category from the path to the file\n",
    "def _getFileCategory(file, catDict):\n",
    "    # Receives a file with name <cat>/<filename> and returns an integer that is catDict[cat]\n",
    "    categ = os.path.basename(os.path.dirname(file))\n",
    "    return catDict.get(categ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categories of each set\n",
    "testWAVlabels = [_getFileCategory(f, DictCategs) for f in testWAVs]\n",
    "valWAVlabels = [_getFileCategory(f, DictCategs) for f in valWAVs]\n",
    "trainWAVlabels = [_getFileCategory(f, DictCategs) for f in trainWAVs]\n",
    "\n",
    "# And test the size of the labels set\n",
    "print(\"Train-Labels set length: \" + str(len(trainWAVlabels)))\n",
    "print(\"Validation-Labels set length: \" + str(len(valWAVlabels)))\n",
    "print(\"Test-Labels set length: \" + str(len(testWAVlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transoform the train dataset in numpy array and load them \n",
    "train = np.array(trainWAVs, dtype = object)\n",
    "trainLabels = np.array(trainWAVlabels, dtype = '>i4') #stands for int32\n",
    "\n",
    "print(\"Loading of the train set:\")\n",
    "for i in range(len(trainWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(trainWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + trainWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + trainWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + trainWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    train[i] = trainWAVs[i] + '.npy'\n",
    "    \n",
    "print(str(i+1) + '/' + str(len(trainWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for the validation and the test set\n",
    "val = np.array(valWAVs, dtype = object)\n",
    "valLabels = np.array(valWAVlabels, dtype = '>i4') #stands for int32\n",
    "\n",
    "print(\"Loading of the validation set:\")\n",
    "for i in range(len(valWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(valWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + valWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + valWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + valWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    val[i] = valWAVs[i] + '.npy'\n",
    "    \n",
    "print(str(i+1) + '/' + str(len(valWAVs)))\n",
    "\n",
    "test = np.array(testWAVs, dtype = object)\n",
    "\n",
    "print(\"Loading of the test set:\")\n",
    "for i in range(len(testWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(testWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + testWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + testWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + testWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    test[i] = testWAVs[i] + '.npy' \n",
    "    \n",
    "print(str(i+1) + '/' + str(len(testWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on the length\n",
    "print(str(len(trainWAVs)) + \" = \" + str(len(train)))\n",
    "print(str(len(valWAVs)) + \" = \" + str(len(val)))\n",
    "print(str(len(testWAVs)) + \" = \" + str(len(test)))\n",
    "\n",
    "#test on labels \n",
    "for i in range(0, 1):\n",
    "    print(\"file: \" + train[i] + \" - label: \" + str(trainLabels[i]))\n",
    "    print(\"file: \" + trainWAVs[i] + \" - label: \" + str(trainLabels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load numpy array\n",
    "def load_data(file_name, data_dir):\n",
    "    # Load the wav signal from the .npy file\n",
    "    data = np.load(data_dir + file_name)\n",
    "    return data\n",
    "\n",
    "# Plot a wav\n",
    "file_name = train[25]\n",
    "data = load_data(file_name, dataset_dir)\n",
    "plt.figure()\n",
    "plt.plot(data, color='b')\n",
    "plt.title('WAV signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data \n",
    "def load_and_preprocess_data(file_name, data_dir):\n",
    "    # Required by tensorflow (strings are passed as bytes)\n",
    "    if type(file_name) is bytes:\n",
    "        file_name = file_name.decode()\n",
    "        data_dir = data_dir.decode()\n",
    "\n",
    "    # Load data\n",
    "    data = load_data(file_name, data_dir)\n",
    "    feats = computeFeatures1(data, 16000)\n",
    "    # Normalize\n",
    "    #feats -= (np.mean(feats, axis=0) + 1e-8)\n",
    "    #mean = np.mean(feats, axis = 0)\n",
    "    #stv = np.std(feats, axis = 0)\n",
    "    #diff = np.subtract(feats, mean)\n",
    "    #feats = np.divide(diff, stv)\n",
    "\n",
    "    return feats.astype(np.float32)\n",
    "\n",
    "# example:\n",
    "index = 26257\n",
    "feats = load_and_preprocess_data(train[index], dataset_dir)\n",
    "feats = np.transpose(feats)\n",
    "#plt.plot(feats, color='b')\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.pcolormesh(feats)\n",
    "\n",
    "plt.title('Spectrogram visualization')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.show()\n",
    "print(\"File: \" + train[index] + \" - Label: \" + str(trainLabels[index]))\n",
    "#print(np.max(feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir, file_names, labels, batch_size = 32, shuffle = True, cache_file = None):\n",
    "    \n",
    "    # Create a Dataset object\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_names, labels))\n",
    "    \n",
    "    # Map the load_and_preprocess_data function\n",
    "    py_func = lambda file_name, label: (tf.py_function(load_and_preprocess_data, [file_name, data_dir], tf.float32), label)\n",
    "    dataset = dataset.map(py_func, num_parallel_calls = os.cpu_count())\n",
    "    \n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "    \n",
    "    # Shuffle    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(file_names))\n",
    "        \n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Correct input shape for the network\n",
    "    dataset = dataset.map(lambda data, label: (tf.expand_dims(data, -1), label))\n",
    "    \n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    \n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size = 1)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = create_dataset(dataset_dir, train, trainLabels, batch_size = batch_size, shuffle = True, cache_file = 'train_cache')\n",
    "\n",
    "val_dataset = create_dataset(dataset_dir, val, valLabels, batch_size = batch_size, shuffle = False, cache_file = 'val_cache')\n",
    "\n",
    "train_steps = int(np.ceil(len(train) / batch_size))\n",
    "val_steps = int(np.ceil(len(val) / batch_size))\n",
    "print(train_steps)\n",
    "print(val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataset for test\n",
    "small_dataset = create_dataset(dataset_dir,\n",
    "                               train[:1000], \n",
    "                               trainLabels[:1000], \n",
    "                               batch_size = batch_size, \n",
    "                               shuffle = True, \n",
    "                               cache_file = 'small_cache')\n",
    "\n",
    "# Define an iterator to get data\n",
    "iterator = small_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "# Start a new session\n",
    "with tf.Session() as sess:\n",
    "    # Iterate for 3 epochs\n",
    "    for num_epoch in range(3):\n",
    "        # Time the loading time\n",
    "        it = time.time()\n",
    "        for step in tqdm(range(3)):\n",
    "                # Get the next batch of data\n",
    "                data, label = sess.run(next_element)\n",
    "        # Print loading time\n",
    "        print('EPOCH {} - Time to load the entire dataset [seconds]: {}'.format(num_epoch+1, time.time() - it))\n",
    "    \n",
    "# Remove the created cache\n",
    "os.remove('small_cache.data-00000-of-00001')\n",
    "os.remove('small_cache.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainGen = SpeechGenerator.SpeechGen(train, trainLabels, dataset_dir, batch_size = 64, shuffle = True)\n",
    "#valGen = SpeechGenerator.SpeechGen(val, valLabels, dataset_dir, batch_size = 16, shuffle = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.backend.set_floatx('float32')\n",
    "\n",
    "#print(keras.backend.floatx())\n",
    "\n",
    "model = Model.SimpleModel(nCategs, 99, 39, use_GRU = True, dropout = 0.1, activation = 'relu')\n",
    "\"\"\"\n",
    "decayed_lr = tf.train.exponential_decay(0.0000001,\n",
    "                                        0.8, 50,\n",
    "                                        0.95, staircase=True)\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate=decayed_lr,  \n",
    "                                beta1=0.9,\n",
    "                                beta2=0.999,\n",
    "                                epsilon=1e-07,\n",
    "                                use_locking=False,\n",
    "                                name='Adam')\n",
    "\"\"\"\n",
    "adam = tf.train.AdamOptimizer(learning_rate=0.001,  \n",
    "                                beta1=0.9,\n",
    "                                beta2=0.999,\n",
    "                                epsilon=1e-07,\n",
    "                                use_locking=False,\n",
    "                                name='Adam')\n",
    "\n",
    "#model.compile(optimizer=adam,\n",
    "#              loss= tf.losses.softmax_cross_entropy,\n",
    "#              metrics=[keras.metrics.sparse_categorical_accuracy, ])\n",
    "\n",
    "model.compile(optimizer = adam,\n",
    "              loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics = ['sparse_categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs = num_epochs, \n",
    "                    steps_per_epoch = train_steps,\n",
    "                    validation_data = val_dataset, \n",
    "                    validation_steps = val_steps)\n",
    "\n",
    "# Save the model\n",
    "model.save('Model/my_model_using-tf.h5')\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['categorical_accuracy'], label='Train acc')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='Val acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.4\n",
    "    epochs_drop = 3.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "            math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    if (lrate < 4e-5):\n",
    "        lrate = 4e-5\n",
    "      \n",
    "    print('Changing learning rate to {}'.format(lrate))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_categorical_accuracy', patience=10, verbose=1)\n",
    "checkpointer = ModelCheckpoint('Model/my_model_loss_dropout-0_05-yes_reg-0.h5', \n",
    "                               monitor='val_categorical_accuracy', \n",
    "                               verbose=1, save_best_only = True, save_weights_only = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "history = model.fit_generator(trainGen, epochs = num_epochs, callbacks = [earlystopper, checkpointer, lrate])\n",
    "\n",
    "print(history.history.keys())\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['categorical_accuracy'], label='Train acc')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='Val acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.get_weights())\n",
    "booo = np.array(model.get_weights())\n",
    "for i in range(len(booo)):\n",
    "    print(str(np.max(booo[i])) + \" - min: \" + str(np.min(booo[i]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging\n",
    "attSpeechModel = keras.Model(inputs=model.input, outputs=[model.get_layer('output').output, \n",
    "                                                    model.get_layer('attSoftmax').output,\n",
    "                                                    model.get_layer('mel_stft').output])\n",
    "\n",
    "gradients = keras.backend.gradients(model.output, model.input)              #Gradient of output wrt the input of the model (Tensor)\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios, classes = valGen.__getitem__(1851)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idAudio = 0\n",
    "print(classes[idAudio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs, attW, specs = attSpeechModel.predict( audios )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boh = np.isnan(specs)\n",
    "np.where(boh == True)\n",
    "print(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (weights only)\n",
    "model.save_weights('Model/my_model_sparse-loss_dropout-0_1-yes_reg-0.h5')\n",
    "# Load model (weights only)\n",
    "#model = keras.models.load_weights(\"Model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGen = SpeechGenerator.SpeechGen(test, testWAVlabels, dataset_dir, shuffle=False)\n",
    "\n",
    "testEval = model.evaluate_generator(testGen, verbose = 1)\n",
    "\n",
    "print(\"loss: \" + str(testEval[0]))\n",
    "print(\"acc: \" + str(testEval[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't now why i need to reshape in this way but it works \n",
    "x = np.load(\"Dataset/tree/022cd682_nohash_1.wav.npy\").reshape((1, 16000))\n",
    "print(x.shape)\n",
    "res = model.predict(x) \n",
    "print(res.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
