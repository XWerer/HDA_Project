{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "#from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import SpeechGenerator\n",
    "import librosa\n",
    "#from keras import losses\n",
    "\n",
    "from extractMFCC import computeFeatures, computeFeatures1\n",
    "from addNoise import addNoise\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tf.executing_eagerly())\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder of the dataset\n",
    "dataset_dir = \"Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing the path that identify the test and validation set\n",
    "testWAVs = pd.read_csv(dataset_dir + 'testing_list.txt', sep=\" \", header = None)[0].tolist()\n",
    "valWAVs  = pd.read_csv(dataset_dir + 'validation_list.txt', sep=\" \", header = None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing the mapping between category name and label\n",
    "DictCategs = {'nine' : 1, 'yes' : 2, 'no' : 3, 'up' : 4, 'down' : 5, 'left' : 6, 'right' : 7, 'on' : 8, 'off' : 9, \n",
    "              'stop' : 10, 'go' : 11, 'zero' : 12, 'one' : 13, 'two' : 14, 'three' : 15, 'four' : 16, 'five' : 17, \n",
    "              'six' : 18, 'seven' : 19, 'eight' : 20, 'backward':0, 'bed':0, 'bird':0, 'cat':0, 'dog':0, 'follow':0, \n",
    "              'forward':0, 'happy':0, 'house':0, 'learn':0, 'marvin':0, 'sheila':0, 'tree':0, 'visual':0, 'wow':0 }\n",
    "nCategs = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the file in dataset\n",
    "allWAVs  = []\n",
    "for root, dirs, files in os.walk('Dataset/'):\n",
    "    for f in files:\n",
    "        if (root != dataset_dir + \"_background_noise_\") and (f.endswith('.wav')):\n",
    "            path = root + \"/\" + f\n",
    "            #print(path)\n",
    "            path = path[len(dataset_dir):]\n",
    "            #print(path)\n",
    "            allWAVs.append(path)\n",
    "\n",
    "# Remove from the training set the elements present in test and validation\n",
    "trainWAVs = list(set(allWAVs) - set(valWAVs) - set(testWAVs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of sets\n",
    "print(\"Train set length: \" + str(len(trainWAVs)))\n",
    "print(\"Validation set length: \" + str(len(valWAVs)))\n",
    "print(\"Test set length: \" + str(len(testWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the category from the path to the file\n",
    "def _getFileCategory(file, catDict):\n",
    "    # Receives a file with name <cat>/<filename> and returns an integer that is catDict[cat]\n",
    "    categ = os.path.basename(os.path.dirname(file))\n",
    "    return catDict.get(categ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categories of each set\n",
    "testWAVlabels = [_getFileCategory(f, DictCategs) for f in testWAVs]\n",
    "valWAVlabels = [_getFileCategory(f, DictCategs) for f in valWAVs]\n",
    "trainWAVlabels = [_getFileCategory(f, DictCategs) for f in trainWAVs]\n",
    "\n",
    "# And test the size of the labels set\n",
    "print(\"Train-Labels set length: \" + str(len(trainWAVlabels)))\n",
    "print(\"Validation-Labels set length: \" + str(len(valWAVlabels)))\n",
    "print(\"Test-Labels set length: \" + str(len(testWAVlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transoform the train dataset in numpy array and load them \n",
    "train = np.array(trainWAVs, dtype = object)\n",
    "trainLabels = np.array(trainWAVlabels, dtype = '>i4') #stands for int32\n",
    "\n",
    "print(\"Loading of the train set:\")\n",
    "for i in range(len(trainWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(trainWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + trainWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + trainWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + trainWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    train[i] = trainWAVs[i] + '.npy'\n",
    "    \n",
    "print(str(i+1) + '/' + str(len(trainWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for the validation and the test set\n",
    "val = np.array(valWAVs, dtype = object)\n",
    "valLabels = np.array(valWAVlabels, dtype = '>i4') #stands for int32\n",
    "\n",
    "print(\"Loading of the validation set:\")\n",
    "for i in range(len(valWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(valWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + valWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + valWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + valWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    val[i] = valWAVs[i] + '.npy'\n",
    "    \n",
    "print(str(i+1) + '/' + str(len(valWAVs)))\n",
    "\n",
    "test = np.array(testWAVs, dtype = object)\n",
    "\n",
    "print(\"Loading of the test set:\")\n",
    "for i in range(len(testWAVs)):\n",
    "    # Print the progress \n",
    "    if (i % 5000) == 0:\n",
    "        print(str(i) + '/' + str(len(testWAVs)))\n",
    "    \n",
    "    # If the file is not already present, we create the numpy version \n",
    "    if (not os.path.isfile(dataset_dir + \"/\" + testWAVs[i] + '.npy')):\n",
    "        y, sr = librosa.load(dataset_dir + \"/\" + testWAVs[i], sr = 16000)\n",
    "        np.save(dataset_dir + \"/\" + testWAVs[i] + '.npy', y)\n",
    "    \n",
    "    # We load the path to numpy array in a vector \n",
    "    test[i] = testWAVs[i] + '.npy' \n",
    "    \n",
    "print(str(i+1) + '/' + str(len(testWAVs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on the length\n",
    "print(str(len(trainWAVs)) + \" = \" + str(len(train)))\n",
    "print(str(len(valWAVs)) + \" = \" + str(len(val)))\n",
    "print(str(len(testWAVs)) + \" = \" + str(len(test)))\n",
    "\n",
    "#test on labels \n",
    "for i in range(0, 1):\n",
    "    print(\"file: \" + train[i] + \" - label: \" + str(trainLabels[i]))\n",
    "    print(\"file: \" + trainWAVs[i] + \" - label: \" + str(trainLabels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load numpy array\n",
    "def load_data(file_name, data_dir):\n",
    "    # Load the wav signal from the .npy file\n",
    "    data = np.load(data_dir + file_name)\n",
    "    return data\n",
    "\n",
    "# Plot a wav\n",
    "file_name = train[25]\n",
    "data = load_data(file_name, dataset_dir)\n",
    "plt.figure()\n",
    "plt.plot(data, color='b')\n",
    "plt.title('WAV signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data \n",
    "def load_and_preprocess_data(file_name, data_dir):\n",
    "    # Required by tensorflow (strings are passed as bytes)\n",
    "    if type(file_name) is bytes:\n",
    "        file_name = file_name.decode()\n",
    "        data_dir = data_dir.decode()\n",
    "\n",
    "    # Load data\n",
    "    data = load_data(file_name, data_dir)\n",
    "    feats = computeFeatures1(data, 16000)\n",
    "    # Normalize\n",
    "    #feats -= np.mean(feats, axis=0)\n",
    "    #mean = np.mean(feats)\n",
    "    #stv = np.std(feats, axis = 0)\n",
    "    #diff = np.subtract(feats, mean)\n",
    "    feats = np.divide(feats, np.max(feats))\n",
    "\n",
    "    return feats.astype(np.float32)\n",
    "    \n",
    "def load_and_preprocess_data2(feats):\n",
    "    # Compute the shifted input \n",
    "    feats = np.delete(feats, 98, 0)\n",
    "    #print(feats.shape)\n",
    "    feats = np.insert(feats, 0, values = 0, axis = 0)\n",
    "    \n",
    "    return feats.astype(np.float32)\n",
    "\n",
    "# example:\n",
    "index = 34587\n",
    "feats = load_and_preprocess_data(train[index], dataset_dir)\n",
    "feats2 = load_and_preprocess_data2(feats)\n",
    "feats = np.transpose(feats)\n",
    "feats2 = np.transpose(feats2)\n",
    "#plt.plot(feats, color='b')\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.pcolormesh(feats)\n",
    "\n",
    "plt.title('Spectrogram visualization')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.pcolormesh(feats2)\n",
    "\n",
    "plt.title('Spectrogram visualization')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.show()\n",
    "print(\"File: \" + train[index] + \" - Label: \" + str(trainLabels[index]))\n",
    "#print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir, file_names, batch_size = 32, shuffle = True, cache_file = None):\n",
    "    \n",
    "    # Create a Dataset object\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_names))\n",
    "    \n",
    "    # Map the load_and_preprocess_data function\n",
    "    py_func = lambda file_name: (tf.py_func(load_and_preprocess_data, [file_name, data_dir], tf.float32))\n",
    "    dataset = dataset.map(py_func, num_parallel_calls = os.cpu_count())\n",
    "    \n",
    "    \n",
    "    # Map the load_and_preprocess_data function\n",
    "    py_func2 = lambda original: (original, \n",
    "                                 tf.py_func(load_and_preprocess_data2, [original], tf.float32),\n",
    "                                 original)\n",
    "    dataset = dataset.map(py_func2, num_parallel_calls = os.cpu_count())\n",
    "    \n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "    \n",
    "    # Shuffle    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(file_names))\n",
    "        \n",
    "    # Repeat the dataset indefinitely (capire bene anche questo repeat come funziona)\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Correct input shape for the network\n",
    "    dataset = dataset.map(lambda data, data2, data3: (tf.expand_dims(data, -1),\n",
    "                                                      data2,\n",
    "                                                      tf.expand_dims(data3, -1)))\n",
    "    \n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    \n",
    "    # Prefetch (1 means that prefetch a batch at time)\n",
    "    dataset = dataset.prefetch(buffer_size = 1)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_dataset(dataset_dir, train, batch_size = batch_size, shuffle = True, cache_file = 'train_cache_AE')\n",
    "\n",
    "val_dataset = create_dataset(dataset_dir, val, batch_size = batch_size, shuffle = False, cache_file = 'val_cache_AE')\n",
    "\n",
    "test_dataset = create_dataset(dataset_dir, test, batch_size = batch_size, shuffle = False, cache_file = 'test_cache_AE')\n",
    "\n",
    "train_steps = int(np.ceil(len(train) / batch_size))\n",
    "val_steps = int(np.ceil(len(val) / batch_size))\n",
    "test_steps = int(np.ceil(len(test) / batch_size))\n",
    "\n",
    "print(\"steps to completa a train epoch: \" + str(train_steps))\n",
    "print(\"steps to completa a validation spoch: \" + str(val_steps))\n",
    "print(\"steps to completa a test epoch: \" + str(test_steps))\n",
    "\n",
    "tf.compat.v1.data.get_output_types(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.Seq2SeqModel(nCategs, 99, 39, use_GRU = True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questa funzione sembra essere difficile da applicare ad un fit normale, si può provare ma intanto ho provato ad usare \n",
    "# l'exp_decay che si da in ingresso all'optimizer\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.4\n",
    "    epochs_drop = 3.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "            math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    if (lrate < 4e-5):\n",
    "        lrate = 4e-5\n",
    "      \n",
    "    print('Changing learning rate to {}'.format(lrate))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience = 3, verbose = 1)\n",
    "checkpointer = ModelCheckpoint('Model/AttentionModel-checkpoint-1.h5', \n",
    "                               monitor = 'val_sparse_categorical_accuracy', \n",
    "                               verbose = 1, save_best_only = True, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "global_step = tf.Variable(train_steps * 2, trainable = False)\n",
    "\n",
    "decayed_lr = tf.train.exponential_decay(learning_rate,\n",
    "                                        global_step, train_steps * 2,\n",
    "                                        0.4, staircase = True)\n",
    "\n",
    "adam = tf.train.AdamOptimizer(decayed_lr, \n",
    "                              beta1 = 0.9,\n",
    "                              beta2 = 0.999,\n",
    "                              epsilon = 1e-07,\n",
    "                              use_locking = False,\n",
    "                              name = 'Adam')\n",
    "\n",
    "model.compile(optimizer = adam,\n",
    "              loss='mse', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 6\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs = num_epochs, \n",
    "                    steps_per_epoch = train_steps,\n",
    "                    validation_data = val_dataset, \n",
    "                    validation_steps = val_steps,\n",
    "                    callbacks = [checkpointer, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history.history.keys())\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label = 'Train loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label = 'Train acc')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'Val acc')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Save the model\n",
    "model.save('Model/AttentionModel-1_00-0107-17.h5')\n",
    "\n",
    "testEval = model.evaluate(test_dataset,\n",
    "                          steps = test_steps,\n",
    "                          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"left/94de6a6a_nohash_2.wav.npy\"\n",
    "x = load_and_preprocess_data(x, dataset_dir).reshape((1, 99, 39, 1))\n",
    "print(x.shape)\n",
    "res = model.predict(x) \n",
    "print(res.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
