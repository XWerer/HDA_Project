{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with a mnist dataset for the models. \n",
    "\n",
    "Import the models and the detaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "4\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "plt.imshow(x_train[2], cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_train[2])\n",
    "\n",
    "#Reshepe the vector for the network\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionModel = Model.AttentionModel(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "attentionModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionModel.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#results = attentionModel.fit(x = x_train, y = y_train, batch_size = 32, epochs = 1, use_multiprocessing = True, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionModel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = attentionModel.predict(x_test[1234].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())\n",
    "print(y_test[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to train the LSTM Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# error here but i don't know why\n",
    "\n",
    "AE = Model.AE(10, 28, 28, use_GRU=True)\n",
    "AE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a replica of the input for the decoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[25])\n",
    "print(x_train.shape)\n",
    "x_train_2 = np.delete(x_train,0,2)\n",
    "print(x_train_2.shape)\n",
    "print(x_train_2[25])\n",
    "x_train_3 = np.insert(x_train_2, 0, values=0, axis=2)\n",
    "print(x_train_3.shape)\n",
    "print(x_train[25])\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "AE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "AE.fit([x_train_noisy, x_train_3], x_train,\n",
    "          batch_size=32,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to save the weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the model\n",
    "# Work only for LSTM\n",
    "#LSTM_Ae.save_weights('prova.h5')\n",
    "#LSTM_Ae.load_weights('Weight_test/prova.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to transfer the weight of the autoencoder to the encoder only and create a FCNN to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_Encoder = Model.AE_Encoder(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "AE_Encoder.summary()\n",
    "\n",
    "#print(LSTM_Ae.layers[3])\n",
    "\"\"\"\n",
    "weights = LSTM_Ae.layers[1].get_weights()\n",
    "LSTM_Encoder.layers[1].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[2].get_weights()\n",
    "LSTM_Encoder.layers[2].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[3].get_weights()\n",
    "LSTM_Encoder.layers[3].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[4].get_weights()\n",
    "LSTM_Encoder.layers[4].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[5].get_weights()\n",
    "LSTM_Encoder.layers[5].set_weights(weights)\n",
    "\"\"\"\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#We need to cahnge this part because if we use gru we have as input 32 elements\n",
    "classifier = models.Sequential()\n",
    "classifier.add(layers.Dense(64, activation='relu', input_shape=(64, )))\n",
    "classifier.add(layers.Dense(32, activation='relu'))\n",
    "classifier.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a encode an image with the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = AE_Encoder.predict(x_train)\n",
    "\n",
    "print(encoding.shape)\n",
    "print(y_train.shape)\n",
    "print(encoding[504])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the classifier with the encode word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(x = encoding, y = y_train, batch_size = 32, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = AE_Encoder.predict(x_test)\n",
    "pred = classifier.evaluate(code, y_test)\n",
    "print(pred)\n",
    "print(y_test[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a test on the CNN - RNN Autoencoder in the same manner of the previous autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 11:23:56.223180 140151701497600 deprecation.py:506] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0704 11:23:56.509634 140151701497600 deprecation.py:506] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0704 11:23:56.512850 140151701497600 deprecation.py:506] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0704 11:23:56.515858 140151701497600 deprecation.py:506] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 5)    30          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 28, 28, 5)    20          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 1)    26          batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 1)    4           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "squeeze_last_dim (Lambda)       (None, 28, 28)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 128), (None, 35712       squeeze_last_dim[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           bidirectional[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           bidirectional[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "remove_last_column (Lambda)     (None, 27, 28)       0           squeeze_last_dim[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           2080        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_zeros_column (Lambda)       (None, 28, 28)       0           remove_last_column[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           2112        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           2112        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 28, 128), (N 35712       add_zeros_column[0][0]           \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 28, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 28, 28)       3612        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_dim (Lambda)                (None, 28, 28, 1)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 1)    4           add_dim[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 10)   60          batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 10)   40          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Conv2D)                 (None, 28, 28, 1)    51          batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 83,655\n",
      "Trainable params: 83,621\n",
      "Non-trainable params: 34\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "S2S, enc = Model.Seq2SeqModel(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "S2S.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 27)\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#print(x_train[25])\n",
    "print(x_train.shape)\n",
    "x_train_2 = np.delete(x_train,0,2)\n",
    "print(x_train_2.shape)\n",
    "#print(x_train_2[25])\n",
    "x_train_3 = np.insert(x_train_2, 0, values=0, axis=2)\n",
    "print(x_train_3.shape)\n",
    "#print(x_train[25])\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 11:24:34.752505 140151701497600 deprecation.py:323] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected size[0] in [0, 0], but got 32\n\t [[{{node remove_last_column/Slice}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-487749fd87f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m S2S.fit(x_train_noisy, x_train,\n\u001b[1;32m     15\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           epochs=1)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected size[0] in [0, 0], but got 32\n\t [[{{node remove_last_column/Slice}}]]"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train_noisy = x_train_noisy.reshape(x_train.shape[0], 28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train_noisy = x_train_noisy.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n",
    "\n",
    "S2S.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "S2S.fit(x_train_noisy, x_train,\n",
    "          batch_size=32,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the model\n",
    "#This part work only for the LSTM part, because the two network are different\n",
    "#S2S.save_weights('Weight_test/provaS2S.h5')\n",
    "#S2S.load_weights('provaS2S.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S_Encoder = Model.Seq2SeqModel_Encoder(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "S2S_Encoder.summary()\n",
    "\"\"\" this part work only for LSTM, not tested the gru version\n",
    "weights = S2S.layers[2].get_weights()\n",
    "S2S_Encoder.layers[2].set_weights(weights)\n",
    "weights = S2S.layers[3].get_weights()\n",
    "S2S_Encoder.layers[3].set_weights(weights)\n",
    "weights = S2S.layers[4].get_weights()\n",
    "S2S_Encoder.layers[4].set_weights(weights)\n",
    "weights = S2S.layers[5].get_weights()\n",
    "S2S_Encoder.layers[5].set_weights(weights)\n",
    "weights = S2S.layers[7].get_weights()\n",
    "S2S_Encoder.layers[7].set_weights(weights)\n",
    "weights = S2S.layers[8].get_weights()\n",
    "S2S_Encoder.layers[8].set_weights(weights)\n",
    "weights = S2S.layers[9].get_weights()\n",
    "S2S_Encoder.layers[9].set_weights(weights)\n",
    "weights = S2S.layers[10].get_weights()\n",
    "S2S_Encoder.layers[10].set_weights(weights)\n",
    "weights = S2S.layers[11].get_weights()\n",
    "S2S_Encoder.layers[11].set_weights(weights)\n",
    "weights = S2S.layers[12].get_weights()\n",
    "S2S_Encoder.layers[12].set_weights(weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to cahnge this part because if we use gru we have as input 32 elements\n",
    "classifier2 = models.Sequential()\n",
    "classifier2.add(layers.Dense(64, activation='relu', input_shape=(64, )))\n",
    "classifier2.add(layers.Dense(48, activation='relu'))\n",
    "classifier2.add(layers.Dense(32, activation='relu'))\n",
    "classifier2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "classifier2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = S2S_Encoder.predict(x_train)\n",
    "\n",
    "print(encoding.shape)\n",
    "print(y_train.shape)\n",
    "print(encoding[504])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "classifier2.fit(x = encoding, y = y_train, batch_size = 128, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = S2S_Encoder.predict(x_test)\n",
    "bo = classifier2.evaluate(code, y_test)\n",
    "print(bo) #this is the cost function \n",
    "print(code[40].shape)\n",
    "test = classifier2.predict(code[5555].reshape(1, 64))\n",
    "print(test.argmax())\n",
    "print(y_test[5555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
