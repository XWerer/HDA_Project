{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with a mnist dataset for the models. \n",
    "\n",
    "Import the models and the detaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "plt.imshow(x_train[2], cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_train[2])\n",
    "\n",
    "#Reshepe the vector for the network\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionModel = Model.AttentionModel(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "attentionModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionModel.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#results = attentionModel.fit(x = x_train, y = y_train, batch_size = 32, epochs = 1, use_multiprocessing = True, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionModel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = attentionModel.predict(x_test[1234].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())\n",
    "print(y_test[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to train the LSTM Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# error here but i don't know why\n",
    "\n",
    "AE = Model.AE(10, 28, 28, use_GRU=True)\n",
    "AE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a replica of the input for the decoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[25])\n",
    "print(x_train.shape)\n",
    "x_train_2 = np.delete(x_train,0,2)\n",
    "print(x_train_2.shape)\n",
    "print(x_train_2[25])\n",
    "x_train_3 = np.insert(x_train_2, 0, values=0, axis=2)\n",
    "print(x_train_3.shape)\n",
    "print(x_train[25])\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "AE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "AE.fit([x_train_noisy, x_train_3], x_train,\n",
    "          batch_size=32,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to save the weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the model\n",
    "# Work only for LSTM\n",
    "#LSTM_Ae.save_weights('prova.h5')\n",
    "#LSTM_Ae.load_weights('Weight_test/prova.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to transfer the weight of the autoencoder to the encoder only and create a FCNN to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_Encoder = Model.AE_Encoder(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "AE_Encoder.summary()\n",
    "\n",
    "#print(LSTM_Ae.layers[3])\n",
    "\"\"\"\n",
    "weights = LSTM_Ae.layers[1].get_weights()\n",
    "LSTM_Encoder.layers[1].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[2].get_weights()\n",
    "LSTM_Encoder.layers[2].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[3].get_weights()\n",
    "LSTM_Encoder.layers[3].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[4].get_weights()\n",
    "LSTM_Encoder.layers[4].set_weights(weights)\n",
    "weights = LSTM_Ae.layers[5].get_weights()\n",
    "LSTM_Encoder.layers[5].set_weights(weights)\n",
    "\"\"\"\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#We need to cahnge this part because if we use gru we have as input 32 elements\n",
    "classifier = models.Sequential()\n",
    "classifier.add(layers.Dense(64, activation='relu', input_shape=(64, )))\n",
    "classifier.add(layers.Dense(32, activation='relu'))\n",
    "classifier.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a encode an image with the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = AE_Encoder.predict(x_train)\n",
    "\n",
    "print(encoding.shape)\n",
    "print(y_train.shape)\n",
    "print(encoding[504])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the classifier with the encode word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(x = encoding, y = y_train, batch_size = 32, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = AE_Encoder.predict(x_test)\n",
    "pred = classifier.evaluate(code, y_test)\n",
    "print(pred)\n",
    "print(y_test[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a test on the CNN - RNN Autoencoder in the same manner of the previous autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "S2S = Model.Seq2SeqModel(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "S2S.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[25])\n",
    "print(x_train.shape)\n",
    "x_train_2 = np.delete(x_train,0,2)\n",
    "print(x_train_2.shape)\n",
    "print(x_train_2[25])\n",
    "x_train_3 = np.insert(x_train_2, 0, values=0, axis=2)\n",
    "print(x_train_3.shape)\n",
    "print(x_train[25])\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train_noisy = x_train_noisy.reshape(x_train.shape[0], 28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train_noisy = x_train_noisy.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n",
    "\n",
    "S2S.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "S2S.fit([x_train_noisy, x_train_3], x_train,\n",
    "          batch_size=32,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the model\n",
    "#This part work only for the LSTM part, because the two network are different\n",
    "#S2S.save_weights('Weight_test/provaS2S.h5')\n",
    "#S2S.load_weights('provaS2S.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S_Encoder = Model.Seq2SeqModel_Encoder(10, x_train.shape[1], x_train.shape[2], use_GRU=True)\n",
    "S2S_Encoder.summary()\n",
    "\"\"\" this part work only for LSTM, not tested the gru version\n",
    "weights = S2S.layers[2].get_weights()\n",
    "S2S_Encoder.layers[2].set_weights(weights)\n",
    "weights = S2S.layers[3].get_weights()\n",
    "S2S_Encoder.layers[3].set_weights(weights)\n",
    "weights = S2S.layers[4].get_weights()\n",
    "S2S_Encoder.layers[4].set_weights(weights)\n",
    "weights = S2S.layers[5].get_weights()\n",
    "S2S_Encoder.layers[5].set_weights(weights)\n",
    "weights = S2S.layers[7].get_weights()\n",
    "S2S_Encoder.layers[7].set_weights(weights)\n",
    "weights = S2S.layers[8].get_weights()\n",
    "S2S_Encoder.layers[8].set_weights(weights)\n",
    "weights = S2S.layers[9].get_weights()\n",
    "S2S_Encoder.layers[9].set_weights(weights)\n",
    "weights = S2S.layers[10].get_weights()\n",
    "S2S_Encoder.layers[10].set_weights(weights)\n",
    "weights = S2S.layers[11].get_weights()\n",
    "S2S_Encoder.layers[11].set_weights(weights)\n",
    "weights = S2S.layers[12].get_weights()\n",
    "S2S_Encoder.layers[12].set_weights(weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to cahnge this part because if we use gru we have as input 32 elements\n",
    "classifier2 = models.Sequential()\n",
    "classifier2.add(layers.Dense(64, activation='relu', input_shape=(64, )))\n",
    "classifier2.add(layers.Dense(48, activation='relu'))\n",
    "classifier2.add(layers.Dense(32, activation='relu'))\n",
    "classifier2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "classifier2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = S2S_Encoder.predict(x_train)\n",
    "\n",
    "print(encoding.shape)\n",
    "print(y_train.shape)\n",
    "print(encoding[504])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "classifier2.fit(x = encoding, y = y_train, batch_size = 128, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = S2S_Encoder.predict(x_test)\n",
    "bo = classifier2.evaluate(code, y_test)\n",
    "print(bo) #this is the cost function \n",
    "print(code[40].shape)\n",
    "test = classifier2.predict(code[5555].reshape(1, 64))\n",
    "print(test.argmax())\n",
    "print(y_test[5555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
